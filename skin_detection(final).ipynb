{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQhj9uuwKmNq",
        "outputId": "165c955b-5060-4b30-e46e-4ea597801a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Paths\n",
        "input_folder = \"/content/drive/MyDrive/skin_Dataset/Skin_Dataset/Segmented pictures\"\n",
        "output_folder = \"/content/drive/MyDrive/skin_Dataset/Skin_Dataset/O-results\"\n",
        "result_folder = \"/content/drive/MyDrive/skin_Dataset/Skin_Dataset/bgraabx\"\n",
        "\n",
        "# Create result folder if it does not exist\n",
        "os.makedirs(result_folder, exist_ok=True)\n",
        "\n",
        "# Image size\n",
        "IMG_HEIGHT, IMG_WIDTH = 256, 256\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_images(folder, is_label=False):\n",
        "    images = []\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE if is_label else cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "        if is_label:\n",
        "            img = img / 255.0  # Normalize to [0, 1]\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "        else:\n",
        "            img = img / 255.0  # Normalize to [0, 1]\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load data\n",
        "X = load_images(input_folder)\n",
        "y = load_images(output_folder, is_label=True)\n",
        "\n",
        "# Define U-Net model\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Bottleneck\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "\n",
        "    # Decoder\n",
        "    u1 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u1 = concatenate([u1, c3])\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    u2 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u2 = concatenate([u2, c2])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u3 = concatenate([u3, c1])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# Build and compile model\n",
        "model = build_unet((IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, validation_split=0.1, epochs=10, batch_size=2,callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "# Predict and save results\n",
        "for i, img in enumerate(X):\n",
        "    prediction = model.predict(np.expand_dims(img, axis=0))[0]\n",
        "    prediction = (prediction > 0.5).astype(np.uint8) * 255\n",
        "    save_path = os.path.join(result_folder, f\"result_{i + 1}.png\")\n",
        "    cv2.imwrite(save_path, prediction)\n"
      ],
      "metadata": {
        "id": "foptqcsBTSe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790562d2-3bfb-495a-c533-cf006dec6e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 10s/step - accuracy: 0.7493 - loss: 0.6917 - val_accuracy: 0.8285 - val_loss: 0.6706 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 10s/step - accuracy: 0.7853 - loss: 0.6826 - val_accuracy: 0.8285 - val_loss: 0.6214 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 10s/step - accuracy: 0.7996 - loss: 0.6520 - val_accuracy: 0.8285 - val_loss: 0.5179 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 10s/step - accuracy: 0.8071 - loss: 0.5856 - val_accuracy: 0.8285 - val_loss: 0.4708 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 11s/step - accuracy: 0.7847 - loss: 0.5532 - val_accuracy: 0.8285 - val_loss: 0.4834 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 10s/step - accuracy: 0.8180 - loss: 0.4468 - val_accuracy: 0.8285 - val_loss: 0.4940 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.8386 - loss: 0.4200\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 10s/step - accuracy: 0.8367 - loss: 0.4223 - val_accuracy: 0.8285 - val_loss: 0.4886 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 10s/step - accuracy: 0.8139 - loss: 0.4412 - val_accuracy: 0.8285 - val_loss: 0.4925 - learning_rate: 5.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 11s/step - accuracy: 0.8514 - loss: 0.4065 - val_accuracy: 0.8285 - val_loss: 0.4762 - learning_rate: 5.0000e-05\n",
            "Epoch 9: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        }
      ]
    }
  ]
}